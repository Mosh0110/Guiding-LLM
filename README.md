# [Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers](https://arxiv.org/abs/2310.18360)

## TL;DR 
This study exposes the vulnerabilities of Large Language Models (LLMs) in Machine Reading Comprehension tasks, focusing on their reliance on shortcuts. The research utilizes GPT-4 both as an editor to insert shortcut triggers in texts and as a reader to respond to questions. The study reveals that GPT-4 can successfully insert shortcut triggers that deceive even itself, resulting in a 15% drop in F1 score. A framework for this editing process is introduced, and we release ShortcutQA, a dataset for future research. Accepted to EMNLP Findings 2023.

## Repository Contents

### Files
- `Eval script.ipynb`: Jupyter notebook for evaluating the performance of LLMs on ShortcutQA.
- `Generate ShortcutQA.ipynb`: Jupyter notebook for generating the ShortcutQA dataset.
- `requirments.txt`: List of Python packages required to run the notebooks.
- `ShortcutQA1.csv`: The initial version of the ShortcutQA dataset.
- `ShortcutQA1_1.csv`: A newer version of the ShortcutQA dataset, recommended for future research.

### How to Use
1. **Installation**: Run `pip install -r requirments.txt` to install all required packages.
2. **Generate Dataset**: Open and run `Generate ShortcutQA.ipynb` to create the ShortcutQA dataset.
3. **Evaluation**: Open and run `Eval script.ipynb` to evaluate an LLM's performance on ShortcutQA.

## Citing This Work
If you find this work useful for your research, please consider citing our paper.

```bibtex
@inproceedings{levy-etal-2023-guiding,
    title = "Guiding {LLM} to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers",
    author = "Levy, Mosh  and
      Ravfogel, Shauli  and
      Goldberg, Yoav",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.569",
    pages = "8495--8505",
    abstract = "Recent applications of LLMs in Machine Reading Comprehension (MRC) systems have shown impressive results, but the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability. We analyze the problem from two angles: LLMs as editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer questions based on the edited text. We introduce a framework that guides an editor to add potential shortcuts-triggers to samples. Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be deceived by its own edits (15{\%} drop in F1). Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a curated dataset generated by our framework for future research.",
}
```

